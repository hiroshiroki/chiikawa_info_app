"""
ã¡ã„ã‹ã‚æƒ…å ±ã¾ã¨ã‚ã‚¢ãƒ—ãƒª
Twitterã€ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã€ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©ã‹ã‚‰æƒ…å ±ã‚’è¡¨ç¤º
"""
import streamlit as st
from supabase import create_client
import json
from datetime import datetime, timedelta

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã¡ã„ã‹ã‚æƒ…å ±ã¾ã¨ã‚",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-title {
        font-size: 2.5rem;
        font-weight: bold;
        color: #FF69B4;
        text-align: center;
        padding: 1rem 0;
    }
    .source-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 12px;
        font-size: 0.85rem;
        font-weight: bold;
        margin-right: 0.5rem;
    }
    .badge-twitter {
        background-color: #1DA1F2;
        color: white;
    }
    .badge-market {
        background-color: #FFB6C1;
        color: white;
    }
    .badge-info {
        background-color: #98D8C8;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# Supabaseæ¥ç¶š
@st.cache_resource
def init_supabase():
    return create_client(
        st.secrets["supabase_url"],
        st.secrets["supabase_key"]
    )

try:
    supabase = init_supabase()
except Exception as e:
    st.error("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼")
    st.info("Streamlit Secretsã« `supabase_url` ã¨ `supabase_key` ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    st.error(f"âš ï¸ è©³ç´°ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# ========================================
# ã‚¿ã‚¤ãƒˆãƒ«
# ========================================

st.markdown('<h1 class="main-title">ğŸ­ ã¡ã„ã‹ã‚æƒ…å ±ã¾ã¨ã‚</h1>', unsafe_allow_html=True)
st.caption("å…¬å¼Twitterã€ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã€ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©ã‹ã‚‰è‡ªå‹•åé›†")

# ========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
# ========================================

with st.sidebar:
    st.header("ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    
    # ã‚«ãƒ†ã‚´ãƒª
    category = st.selectbox(
        "ã‚«ãƒ†ã‚´ãƒª",
        ["ã™ã¹ã¦", "ã‚°ãƒƒã‚º", "ãã˜", "ã‚¤ãƒ™ãƒ³ãƒˆ", "æ¼«ç”»", "ã‚¢ãƒ‹ãƒ¡", "ãã®ä»–"],
        help="ã‚«ãƒ†ã‚´ãƒªã§çµã‚Šè¾¼ã¿"
    )
    
    # æƒ…å ±æº
    source_options = {
        "twitter": "ğŸ¦ Twitter",
        "chiikawa_market": "ğŸ ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ",
        "chiikawa_info": "ğŸ“° ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©"
    }
    
    selected_sources = st.multiselect(
        "æƒ…å ±æº",
        list(source_options.keys()),
        default=list(source_options.keys()),
        format_func=lambda x: source_options[x],
        help="è¡¨ç¤ºã™ã‚‹æƒ…å ±æºã‚’é¸æŠ"
    )
    
    # æœŸé–“
    period = st.selectbox(
        "æœŸé–“",
        ["ã™ã¹ã¦", "24æ™‚é–“ä»¥å†…", "3æ—¥ä»¥å†…", "1é€±é–“ä»¥å†…", "1ãƒ¶æœˆä»¥å†…"],
        help="æŠ•ç¨¿æ—¥ã§çµã‚Šè¾¼ã¿"
    )
    
    # æ¤œç´¢
    search_text = st.text_input(
        "ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢",
        placeholder="ä¾‹: ã¬ã„ãã‚‹ã¿ã€ã‚¤ãƒ™ãƒ³ãƒˆ",
        help="ã‚¿ã‚¤ãƒˆãƒ«ã‚„æœ¬æ–‡ã‚’æ¤œç´¢"
    )
    
    # ç”»åƒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    only_with_images = st.checkbox(
        "ğŸ“¸ ç”»åƒã‚ã‚Šã®ã¿è¡¨ç¤º",
        value=False,
        help="ç”»åƒãŒå«ã¾ã‚Œã‚‹æŠ•ç¨¿ã®ã¿è¡¨ç¤º"
    )
    
    st.divider()
    
    # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒœã‚¿ãƒ³
    if st.button("ğŸ”„ æ›´æ–°", use_container_width=True):
        st.cache_data.clear()
        st.rerun()

# ========================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—
# ========================================

@st.cache_data(ttl=300)  # 5åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_information(category, sources, period, search, only_images):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’å–å¾—"""
    query = supabase.table("information").select("*")
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    if category != "ã™ã¹ã¦":
        query = query.eq("category", category)
    
    # æƒ…å ±æºãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    if sources:
        query = query.in_("source", sources)
    
    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    if period != "ã™ã¹ã¦":
        days_map = {
            "24æ™‚é–“ä»¥å†…": 1,
            "3æ—¥ä»¥å†…": 3,
            "1é€±é–“ä»¥å†…": 7,
            "1ãƒ¶æœˆä»¥å†…": 30
        }
        date_from = (datetime.now() - timedelta(days=days_map[period])).isoformat()
        query = query.gte("published_at", date_from)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    if search:
        query = query.or_(f"title.ilike.%{search}%,content.ilike.%{search}%")
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    data = query.order("published_at", desc=True).limit(200).execute()
    
    results = data.data
    
    # ç”»åƒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã§å®Ÿæ–½ï¼‰
    if only_images:
        results = [
            r for r in results
            if r.get('images') and json.loads(r['images'])
        ]
    
    return results

# ãƒ‡ãƒ¼ã‚¿å–å¾—å®Ÿè¡Œ
try:
    info_list = get_information(
        category,
        selected_sources,
        period,
        search_text,
        only_with_images
    )
except Exception as e:
    st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    info_list = []

# ========================================
# çµ±è¨ˆè¡¨ç¤º
# ========================================

st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric("ç·ä»¶æ•°", len(info_list))

with col2:
    twitter_count = len([i for i in info_list if i['source'] == 'twitter'])
    st.metric("ğŸ¦ Twitter", twitter_count)

with col3:
    market_count = len([i for i in info_list if i['source'] == 'chiikawa_market'])
    st.metric("ğŸ ãƒãƒ¼ã‚±ãƒƒãƒˆ", market_count)

with col4:
    info_count = len([i for i in info_list if i['source'] == 'chiikawa_info'])
    st.metric("ğŸ“° ã‚¤ãƒ³ãƒ•ã‚©", info_count)

st.divider()

# ========================================
# æƒ…å ±ä¸€è¦§è¡¨ç¤º
# ========================================

if not info_list:
    st.info("ğŸ“­ è©²å½“ã™ã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
    st.write("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„")
else:
    st.subheader(f"ğŸ“° æœ€æ–°æƒ…å ± ({len(info_list)}ä»¶)")
    
    for idx, item in enumerate(info_list):
        with st.container():
            # ã‚¢ã‚¤ã‚³ãƒ³ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¨ªä¸¦ã³
            col_icon, col_content = st.columns([1, 20])
            
            with col_icon:
                # æƒ…å ±æºåˆ¥ã‚¢ã‚¤ã‚³ãƒ³
                source_icons = {
                    "twitter": "ğŸ¦",
                    "chiikawa_market": "ğŸ",
                    "chiikawa_info": "ğŸ“°"
                }
                st.markdown(f"### {source_icons.get(item['source'], 'ğŸ“Œ')}")
            
            with col_content:
                # ã‚¿ã‚¤ãƒˆãƒ«
                st.markdown(f"### {item['title']}")
                
                # ãƒ¡ã‚¿æƒ…å ±ï¼ˆæ—¥ä»˜ã€ã‚«ãƒ†ã‚´ãƒªã€ã‚½ãƒ¼ã‚¹ï¼‰
                meta_col1, meta_col2, meta_col3 = st.columns([2, 1, 2])
                
                with meta_col1:
                    # æ—¥ä»˜
                    try:
                        pub_date = item['published_at']
                        if isinstance(pub_date, str):
                            date_str = pub_date[:10] if len(pub_date) >= 10 else pub_date
                        else:
                            date_str = str(pub_date)[:10]
                        st.caption(f"ğŸ“… {date_str}")
                    except:
                        st.caption("ğŸ“… æ—¥ä»˜ä¸æ˜")
                
                with meta_col2:
                    # ã‚«ãƒ†ã‚´ãƒª
                    category_emoji = {
                        "ã‚°ãƒƒã‚º": "ğŸ",
                        "ãã˜": "ğŸ²",
                        "ã‚¤ãƒ™ãƒ³ãƒˆ": "ğŸª",
                        "æ¼«ç”»": "ğŸ“–",
                        "ã‚¢ãƒ‹ãƒ¡": "ğŸ“º",
                        "ãã®ä»–": "ğŸ“Œ"
                    }
                    emoji = category_emoji.get(item['category'], "ğŸ“Œ")
                    st.caption(f"{emoji} {item['category']}")
                
                with meta_col3:
                    # æƒ…å ±æº
                    source_names = {
                        "twitter": "ğŸ¦ Twitter",
                        "chiikawa_market": "ğŸ ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ",
                        "chiikawa_info": "ğŸ“° ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©"
                    }
                    st.caption(f"ğŸ“ {source_names.get(item['source'], item['source'])}")
                
                # æœ¬æ–‡ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã¨ç•°ãªã‚‹å ´åˆã®ã¿ï¼‰
                if item.get('content') and item['content'] != item['title']:
                    content_text = item['content']
                    # HTMLã‚¿ã‚°ã‚’é™¤å»
                    from bs4 import BeautifulSoup
                    content_text = BeautifulSoup(content_text, 'html.parser').get_text()
                    
                    # é•·ã™ãã‚‹å ´åˆã¯çœç•¥
                    if len(content_text) > 300:
                        content_text = content_text[:300] + "..."
                    
                    if content_text.strip():
                        st.write(content_text)
                
                # ç”»åƒè¡¨ç¤º
                if item.get('images'):
                    try:
                        images = json.loads(item['images']) if isinstance(item['images'], str) else item['images']
                        
                        if images and len(images) > 0:
                            st.caption(f"ğŸ“¸ ç”»åƒ ({len(images)}æš)")
                            
                            # ç”»åƒã‚’æ¨ªä¸¦ã³ã§è¡¨ç¤º
                            if len(images) == 1:
                                st.image(images[0], width=400)
                            elif len(images) == 2:
                                img_cols = st.columns(2)
                                for i, img_url in enumerate(images):
                                    with img_cols[i]:
                                        st.image(img_url, use_container_width=True)
                            else:
                                # 3æšä»¥ä¸Šã¯3åˆ—ã§è¡¨ç¤º
                                img_cols = st.columns(3)
                                for i, img_url in enumerate(images[:6]):  # æœ€å¤§6æš
                                    with img_cols[i % 3]:
                                        st.image(img_url, use_container_width=True)
                    except Exception as e:
                        st.caption(f"âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")
                
                # ãƒªãƒ³ã‚¯ãƒœã‚¿ãƒ³
                st.link_button(
                    "ğŸ”— å…ƒè¨˜äº‹ã‚’è¦‹ã‚‹",
                    item['url'],
                    use_container_width=False
                )
            
            # åŒºåˆ‡ã‚Šç·š
            if idx < len(info_list) - 1:
                st.divider()

# ========================================
# ãƒ•ãƒƒã‚¿ãƒ¼
# ========================================

st.divider()
st.caption("ğŸ’¡ æƒ…å ±ã¯è‡ªå‹•åé›†ã•ã‚Œã¾ã™ã€‚æœ€æ–°æƒ…å ±ã¯å„å…¬å¼ã‚µã‚¤ãƒˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
st.caption("Â©nagano / chiikawa committee")