"""
ã¡ã„ã‹ã‚æƒ…å ±åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Twitter(Nitter), ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ, ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©ã‹ã‚‰æƒ…å ±ã‚’è‡ªå‹•åé›†
"""
import os
import sys
import time
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
    from bs4 import BeautifulSoup
    import feedparser
    from supabase import create_client, Client
except ImportError as e:
    print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ========================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================

def generate_source_id(text: str) -> str:
    """æ–‡å­—åˆ—ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(text.encode()).hexdigest()

def classify_content(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ¤å®š"""
    keywords = {
        "ã‚°ãƒƒã‚º": ["ã‚°ãƒƒã‚º", "ç™ºå£²", "äºˆç´„", "è²©å£²", "é™å®š", "ã¬ã„ãã‚‹ã¿", "ãƒ•ã‚£ã‚®ãƒ¥ã‚¢", "ãƒã‚¹ã‚³ãƒƒãƒˆ", "ã‚¢ã‚¯ã‚¹ã‚¿"],
        "ãã˜": ["ä¸€ç•ªãã˜", "ãã˜", "ãƒ­ãƒƒãƒˆ", "æ™¯å“"],
        "ã‚¤ãƒ™ãƒ³ãƒˆ": ["ã‚¤ãƒ™ãƒ³ãƒˆ", "é–‹å‚¬", "ã‚³ãƒ©ãƒœ", "ã‚«ãƒ•ã‚§", "ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—", "å±•ç¤º", "ã‚‰ã‚“ã©"],
        "æ¼«ç”»": ["æ›´æ–°", "æ²è¼‰", "é€£è¼‰", "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰", "è©±"],
        "ã‚¢ãƒ‹ãƒ¡": ["æ”¾é€", "é…ä¿¡", "å£°å„ª", "OP", "ED"],
    }
    
    for category, words in keywords.items():
        if any(word in text for word in words):
            return category
    
    return "ãã®ä»–"

def save_to_db(items: List[Dict], source: str) -> int:
    """æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    saved_count = 0
    
    for item in items:
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ (source_idã‚’ä½¿ç”¨)
            existing = supabase.table("information").select("id").eq("source_id", item['source_id']).execute()
            
            if not existing.data:
                data = {
                    "source": source,
                    "source_id": item['source_id'],
                    "title": item['title'],
                    "content": item.get('content', item['title']),
                    "url": item['url'],
                    "images": item.get('images', []), # listã®ã¾ã¾æ¸¡ã™ï¼ˆsupabase-pyãŒè‡ªå‹•ã§JSONBã«å¤‰æ›ï¼‰
                    "price": item.get('price'),
                    "category": classify_content(item['title']),
                    "published_at": item.get('published_at', datetime.now().isoformat())
                }
                
                supabase.table("information").insert(data).execute()
                saved_count += 1
                img_count = len(item.get('images', []))
                print(f"  âœ… ä¿å­˜: {item['title'][:30]}... (ç”»åƒ{img_count}æš)")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        time.sleep(0.1) 
    return saved_count

# ========================================
# 1. Twitteråé›†ï¼ˆNitter RSSï¼‰
# ========================================

def collect_twitter() -> List[Dict]:
    print("\nğŸ¦ Twitteråé›†é–‹å§‹...")
    nitter_instances = ["https://nitter.poast.org", "https://nitter.privacydev.net"]
    account = "ngnchiikawa"
    
    for instance in nitter_instances:
        try:
            rss_url = f"{instance}/{account}/rss"
            feed = feedparser.parse(rss_url)
            if feed.entries:
                results = []
                for entry in feed.entries[:20]:
                    images = []
                    # summaryå†…ã®ç”»åƒã‚¿ã‚°ã‚’æŠ½å‡º
                    if hasattr(entry, 'summary'):
                        soup = BeautifulSoup(entry.summary, 'html.parser')
                        for img in soup.find_all('img'):
                            src = img.get('src')
                            if src:
                                if src.startswith('//'): src = f"https:{src}"
                                images.append(src)
                    
                    tweet_id = entry.link.split("/")[-1].split("#")[0]
                    results.append({
                        'source_id': f"twitter_{tweet_id}",
                        'title': entry.title[:100],
                        'content': entry.get('summary', entry.title),
                        'url': entry.link.replace(instance, "https://twitter.com"),
                        'images': images,
                        'published_at': datetime.now().isoformat() # RSSã®æ—¥ä»˜å½¢å¼ã¯å¤šæ§˜ãªãŸã‚ç°¡æ˜“åŒ–
                    })
                return results
        except Exception as e:
            continue
    return []

# ========================================
# 2. ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†ï¼ˆå¼·åŒ–ç‰ˆï¼‰
# ========================================

def collect_chiikawa_market() -> List[Dict]:
    print("\nğŸ ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†é–‹å§‹...")
    url = "https://chiikawamarket.jp/collections/newitems"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ã‚»ãƒ¬ã‚¯ã‚¿ã®æ”¹å–„ï¼šå•†å“ã‚«ãƒ¼ãƒ‰ã‚’æ­£ç¢ºã«å–å¾—
        items = soup.select('.product-item, .card')
        results = []
        seen_ids = set()

        for item in items:
            title_elem = item.select_one('.product-item__title, .card__title, h3')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            
            link_elem = item.select_one('a[href*="/products/"]')
            if not link_elem: continue
            product_url = link_elem.get('href')
            if not product_url.startswith('http'):
                product_url = f"https://chiikawamarket.jp{product_url}"

            # é‡è¤‡é˜²æ­¢ãƒ­ã‚¸ãƒƒã‚¯ï¼šURLã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸€æ„ã®IDã‚’ä½œã‚‹
            source_id = generate_source_id(f"{product_url}_{title}")
            if source_id in seen_ids: continue
            seen_ids.add(source_id)

            # ç”»åƒå–å¾—ã®å¼·åŒ– (Lazy Loadå¯¾å¿œ)
            images = []
            img_tag = item.select_one('img')
            if img_tag:
                img_url = img_tag.get('data-src') or img_tag.get('src') or img_tag.get('data-lazy-src')
                if not img_url and img_tag.get('srcset'):
                    img_url = img_tag.get('srcset').split(',')[0].split(' ')[0]
                
                if img_url:
                    if img_url.startswith('//'): img_url = f"https:{img_url}"
                    elif not img_url.startswith('http'): img_url = f"https://chiikawamarket.jp{img_url}"
                    images.append(img_url.split('?')[0]) # ã‚¯ã‚¨ãƒªå‰Šé™¤

            # é‡‘é¡å–å¾—
            price = None
            price_elem = item.select_one('.price, .price-item')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                # "Â¥", "å††", "," ã‚’é™¤å»ã—ã¦æ•°å€¤ã«å¤‰æ›
                try:
                    price = int("".join(filter(str.isdigit, price_text)))
                except ValueError:
                    price = None

            results.append({
                'source_id': source_id,
                'title': title,
                'url': product_url,
                'images': images,
                'price': price,
                'published_at': datetime.now().isoformat()
            })
            if len(results) >= 20: break

        print(f"  âœ… {len(results)}ä»¶è§£æå®Œäº†")
        return results
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# 3. ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†
# ========================================

def collect_chiikawa_info() -> List[Dict]:
    print("\nğŸ“° ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†é–‹å§‹...")
    url = "https://chiikawa-info.jp/"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        items = soup.select('.news-item, article, li')
        results = []
        for item in items:
            title_elem = item.select_one('h2, h3, .title')
            link_elem = item.select_one('a')
            if not title_elem or not link_elem: continue
            
            title = title_elem.get_text(strip=True)
            info_url = link_elem.get('href')
            if not info_url or len(title) < 5: continue
            if not info_url.startswith('http'):
                info_url = f"https://chiikawa-info.jp{info_url}"

            images = []
            img_tag = item.select_one('img')
            if img_tag:
                src = img_tag.get('src')
                if src:
                    if src.startswith('//'): src = f"https:{src}"
                    elif not src.startswith('http'): src = f"https://chiikawa-info.jp{src}"
                    images.append(src)

            results.append({
                'source_id': generate_source_id(info_url),
                'title': title,
                'url': info_url,
                'images': images,
                'published_at': datetime.now().isoformat()
            })
            if len(results) >= 20: break
        return results
    except Exception as e:
        return []

# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    print(f"ğŸš€ å®Ÿè¡Œé–‹å§‹: {datetime.now()}")
    total_saved = 0
    
    # é †æ¬¡å®Ÿè¡Œ
    for source_name, collector in [
        ("twitter", collect_twitter),
        ("chiikawa_market", collect_chiikawa_market),
        ("chiikawa_info", collect_chiikawa_info)
    ]:
        items = collector()
        if items:
            saved = save_to_db(items, source_name)
            print(f"  ğŸ“Š {source_name}: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
            total_saved += saved
        time.sleep(1)

    print(f"\nâœ¨ å®Œäº†ï¼åˆè¨ˆ {total_saved} ä»¶ã®æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()