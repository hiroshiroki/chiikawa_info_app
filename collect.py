"""
ã¡ã„ã‹ã‚æƒ…å ±åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã‹ã‚‰æ–°å•†å“ãƒ»å†å…¥è·æƒ…å ±ã‚’è‡ªå‹•åé›†
"""
import os
import sys
import time
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
import re
import pytz

try:
    import requests
    from bs4 import BeautifulSoup
    from supabase import create_client, Client
    from notifier import DiscordNotifier
except ImportError as e:
    print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# å®šæ•°
BASE_URL = "https://chiikawamarket.jp"
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
TOKYO_TZ = pytz.timezone('Asia/Tokyo')


def generate_source_id(text: str) -> str:
    """æ–‡å­—åˆ—ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(text.encode()).hexdigest()


def check_restock(item: Dict) -> None:
    """
    å†å…¥è·ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å±¥æ­´ã«è¨˜éŒ²

    Args:
        item: ãƒã‚§ãƒƒã‚¯ã™ã‚‹å•†å“ã‚¢ã‚¤ãƒ†ãƒ 
    """
    try:
        # status='restock'ã§ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
        if item.get('status') != 'restock':
            return

        # åŒã˜URLã®æ—¢å­˜å•†å“ã‚’æ¤œç´¢
        existing = supabase.table("information").select("*").eq("url", item['url']).execute()

        # å†å…¥è·å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        previous_event_date = None
        if existing.data:
            existing_item = existing.data[0]
            previous_event_date = existing_item.get('event_date')

        new_event_date = item.get('event_date')

        # åŒã˜URLã®æœªé€šçŸ¥ã®å†å…¥è·å±¥æ­´ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
        existing_restock = supabase.table("restock_history")\
            .select("*")\
            .eq("product_url", item['url'])\
            .eq("notified", False)\
            .execute()

        if existing_restock.data:
            # æ—¢ã«æœªé€šçŸ¥ã®å†å…¥è·å±¥æ­´ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            print(f"  â„¹ï¸ æ—¢ã«æœªé€šçŸ¥ã®å†å…¥è·å±¥æ­´ã‚ã‚Š: {item['title'][:30]}...")
            return

        # å†å…¥è·å±¥æ­´ã«è¨˜éŒ²ï¼ˆåˆå›åé›†ã§ã‚‚æ—¢å­˜å•†å“ãŒã‚ã£ã¦ã‚‚è¨˜éŒ²ã™ã‚‹ï¼‰
        restock_data = {
            "product_url": item['url'],
            "product_title": item['title'],
            "previous_event_date": previous_event_date,
            "new_event_date": new_event_date,
            "detected_at": datetime.now(TOKYO_TZ).isoformat()
        }

        supabase.table("restock_history").insert(restock_data).execute()
        is_new = not existing.data
        print(f"  ğŸ”” å†å…¥è·æ¤œå‡º: {item['title'][:30]}... (åˆå›åé›†: {is_new})")

        # æ—¢å­˜å•†å“ã®statusã¨event_dateã‚’restockã«æ›´æ–°
        if existing.data:
            supabase.table("information")\
                .update({"status": "restock", "event_date": new_event_date})\
                .eq("id", existing.data[0]['id'])\
                .execute()
            print(f"  âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°: {existing.data[0]['id']}")

    except Exception as e:
        print(f"  âš ï¸ å†å…¥è·ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {item.get('title', 'ä¸æ˜ãªã‚¢ã‚¤ãƒ†ãƒ ')} - {e}")


def save_to_db(items: List[Dict], source: str) -> int:
    """
    æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜

    Args:
        items: ä¿å­˜ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        source: æƒ…å ±æºï¼ˆ'chiikawa_market'ç­‰ï¼‰

    Returns:
        ä¿å­˜ä»¶æ•°
    """
    saved_count = 0

    # æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒå…ˆã«æ¥ã‚‹ã‚ˆã†ã«é€†é †ã§å‡¦ç†
    for item in reversed(items):
        try:
            # å†å…¥è·ãƒã‚§ãƒƒã‚¯ï¼ˆä¿å­˜å‰ã«å®Ÿè¡Œï¼‰
            check_restock(item)

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            existing = supabase.table("information").select("id").eq("source_id", item['source_id']).execute()

            if not existing.data:
                data = {
                    "source": source,
                    "source_id": item['source_id'],
                    "title": item['title'],
                    "content": item.get('content', item['title']),
                    "url": item['url'],
                    "images": item.get('images', []),
                    "price": item.get('price'),
                    "category": "ã‚°ãƒƒã‚º",
                    "published_at": item.get('published_at', datetime.now(TOKYO_TZ).isoformat()),
                    "status": item.get('status', 'new'),
                    "event_date": item.get('event_date')
                }

                supabase.table("information").insert(data).execute()
                saved_count += 1
                img_count = len(item.get('images', []))
                print(f"  âœ… ä¿å­˜: {item['title'][:30]}... (ç”»åƒ{img_count}æš)")

        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {item.get('title', 'ä¸æ˜ãªã‚¢ã‚¤ãƒ†ãƒ ')} - {e}")

        time.sleep(0.1)

    return saved_count


def extract_event_date(date_text: Optional[str], url: str) -> Optional[str]:
    """
    æ—¥ä»˜ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯URLã‹ã‚‰ç™ºå£²æ—¥ãƒ»å†å…¥è·æ—¥ã‚’æŠ½å‡º

    Args:
        date_text: æ—¥ä»˜ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹: "2æœˆ6æ—¥ç™ºå£²å•†å“"ï¼‰
        url: URLï¼ˆä¾‹: /collections/20260206ï¼‰

    Returns:
        æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰ã¾ãŸã¯None
    """
    # 1. date_textã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
    if date_text:
        match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', date_text)
        if match:
            month, day = int(match.group(1)), int(match.group(2))
            now = datetime.now(TOKYO_TZ)
            year = now.year if month <= now.month + 1 else now.year - 1
            try:
                event_date_str = datetime(year, month, day).strftime('%Y-%m-%d')
                print(f"  ğŸ“… ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ã‚’æŠ½å‡º: {event_date_str} (from: {date_text})")
                return event_date_str
            except ValueError:
                pass

    # 2. URLã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
    url_date_match = re.search(r'/collections/(?:re)?(\d{8})', url)
    if url_date_match:
        date_str = url_date_match.group(1)
        try:
            date_obj = datetime.strptime(date_str, '%Y%m%d')
            event_date_str = date_obj.strftime('%Y-%m-%d')
            print(f"  ğŸ“… ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ã‚’æŠ½å‡º: {event_date_str} (from URL: {url})")
            return event_date_str
        except ValueError:
            pass

    return None


def get_latest_market_urls() -> List[Dict[str, str]]:
    """
    ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã®æ—¥ä»˜åˆ¥ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—

    Returns:
        URLãƒªã‚¹ãƒˆï¼ˆurl, status, date_textã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆï¼‰
    """
    print("  ğŸ”— æ—¥ä»˜åˆ¥ãƒãƒ¼ã‚±ãƒƒãƒˆURLã‚’å–å¾—ä¸­...")
    collections = []
    seen_urls = set()

    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.get(BASE_URL, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
        for link in soup.select('a[href*="/collections/"]'):
            href = link.get('href')
            text = link.get_text(strip=True)

            if not href:
                continue

            full_url = f"{BASE_URL}{href}" if not href.startswith('http') else href

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if full_url in seen_urls:
                continue

            # æ–°å•†å“ãƒªãƒ³ã‚¯ï¼ˆä¾‹: "2æœˆ6æ—¥ç™ºå£²å•†å“"ï¼‰
            if 'ç™ºå£²å•†å“' in text:
                date_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
                if date_match:
                    collections.append({
                        'url': full_url,
                        'status': 'new',
                        'date_text': text
                    })
                    seen_urls.add(full_url)

            # å†å…¥è·ãƒªãƒ³ã‚¯ï¼ˆä¾‹: "2æœˆ5æ—¥å†å…¥è·å•†å“"ï¼‰
            elif 'å†å…¥è·å•†å“' in text and 'å†å…¥è·å•†å“ä¸€è¦§' not in text:
                date_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
                if date_match:
                    collections.append({
                        'url': full_url,
                        'status': 'restock',
                        'date_text': text
                    })
                    seen_urls.add(full_url)

        if collections:
            print(f"  ğŸ‘ å–å¾—æˆåŠŸ: {len(collections)}å€‹ã®æ—¥ä»˜åˆ¥ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹")
            for col in collections:
                print(f"    - {col['date_text']}: {col['url']}")
        else:
            print("  âš ï¸ æ—¥ä»˜åˆ¥ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        return collections

    except Exception as e:
        print(f"  âŒ ãƒãƒ¼ã‚±ãƒƒãƒˆURLã®å–å¾—ã«å¤±æ•—: {e}")
        return []


def collect_chiikawa_market(url: str, status: str, date_text: Optional[str] = None) -> List[Dict]:
    """
    ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã‹ã‚‰å•†å“æƒ…å ±ã‚’åé›†

    Args:
        url: åé›†å¯¾è±¡ã®URL
        status: å•†å“åŒºåˆ†ï¼ˆ'new' or 'restock'ï¼‰
        date_text: æ—¥ä»˜ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        å•†å“æƒ…å ±ã®ãƒªã‚¹ãƒˆ
    """
    if not url:
        print(f"  ({status}) ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    print(f"  ({status}) åé›†é–‹å§‹: {url}")

    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ã®æŠ½å‡º
        event_date_str = extract_event_date(date_text, url)

        results = []
        items = soup.select('.card-wrapper, .product-grid .grid__item')

        for item in items:
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_elem = item.select_one('.card__heading, .card-information__text')
            if not title_elem:
                continue
            title = title_elem.get_text(strip=True)

            # URLå–å¾—
            link_elem = item.select_one('a[href*="/products/"]')
            if not link_elem:
                continue
            product_url = link_elem.get('href')
            if not product_url.startswith('http'):
                product_url = f"{BASE_URL}{product_url.split('?')[0]}"

            # ãƒ¦ãƒ‹ãƒ¼ã‚¯IDç”Ÿæˆ
            source_id = generate_source_id(f"{product_url}_{title}")

            # ç”»åƒå–å¾—
            images = []
            img_tag = item.select_one('.card__media img, .media img')
            if img_tag:
                img_url = img_tag.get('src') or img_tag.get('data-src')
                if not img_url and img_tag.get('srcset'):
                    img_url = re.split(r'\s*,\s*', img_tag.get('srcset'))[0].split(' ')[0]

                if img_url:
                    if img_url.startswith('//'):
                        img_url = f"https:{img_url}"
                    images.append(img_url.split('?')[0])

            # ä¾¡æ ¼å–å¾—
            price = None
            price_elem = item.select_one('.price__regular .price-item, .price-item--regular')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                match = re.search(r'(\d[\d,.]*)', price_text)
                if match:
                    try:
                        price = int(float(match.group(1).replace(',', '')))
                    except ValueError:
                        pass

            results.append({
                'source_id': source_id,
                'title': title,
                'url': product_url,
                'images': images,
                'price': price,
                'published_at': datetime.now(TOKYO_TZ).isoformat(),
                'status': status,
                'event_date': event_date_str
            })

        print(f"  âœ… {len(results)}ä»¶è§£æå®Œäº†")
        return results

    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print(f"ğŸš€ å®Ÿè¡Œé–‹å§‹: {datetime.now(TOKYO_TZ)}")
    total_saved = 0

    # é€šçŸ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–
    notifier = DiscordNotifier()

    # æ—¥ä»˜åˆ¥ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    collections = get_latest_market_urls()

    if not collections:
        print("  âš ï¸ åé›†ã™ã‚‹ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æƒ…å ±ã‚’åé›†
    all_items = []
    for collection in collections:
        status_label = "æ–°å•†å“" if collection['status'] == 'new' else "å†å…¥è·"
        print(f"\n--- chiikawa_market ({status_label}: {collection['date_text']}) åé›† ---")
        items = collect_chiikawa_market(
            collection['url'],
            collection['status'],
            collection['date_text']
        )
        all_items.extend(items)
        time.sleep(1)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    if all_items:
        print("\n--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ ---")
        saved = save_to_db(all_items, "chiikawa_market")
        print(f"  ğŸ“Š chiikawa_market: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print("  âš ï¸ chiikawa_marketã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # æœªé€šçŸ¥ã®å†å…¥è·æƒ…å ±ã‚’å–å¾—ã—ã¦é€šçŸ¥
    print("\n--- å†å…¥è·é€šçŸ¥ ---")
    try:
        unnotified = supabase.table("restock_history").select("*").eq("notified", False).execute()

        if unnotified.data:
            print(f"  ğŸ“¬ æœªé€šçŸ¥ã®å†å…¥è·: {len(unnotified.data)}ä»¶")

            # Discordé€šçŸ¥é€ä¿¡
            if notifier.send_restock_notification(unnotified.data):
                print(f"  âœ… Discordé€šçŸ¥é€ä¿¡æˆåŠŸ")

                # é€šçŸ¥æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’æ›´æ–°
                for item in unnotified.data:
                    supabase.table("restock_history").update({"notified": True}).eq("id", item['id']).execute()
                print(f"  âœ… é€šçŸ¥ãƒ•ãƒ©ã‚°æ›´æ–°å®Œäº†")
            else:
                if notifier.enabled:
                    print(f"  âš ï¸ Discordé€šçŸ¥é€ä¿¡å¤±æ•—ï¼ˆé€šçŸ¥ãƒ•ãƒ©ã‚°ã¯æ›´æ–°ã—ã¾ã›ã‚“ï¼‰")
                else:
                    print(f"  â„¹ï¸ Discordé€šçŸ¥ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        else:
            print(f"  â„¹ï¸ æœªé€šçŸ¥ã®å†å…¥è·ã¯ã‚ã‚Šã¾ã›ã‚“")

    except Exception as e:
        print(f"  âš ï¸ å†å…¥è·é€šçŸ¥å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    # ã‚µãƒãƒªãƒ¼é€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if notifier.enabled and os.getenv("DISCORD_SEND_SUMMARY", "false").lower() == "true":
        restock_count = len(unnotified.data) if unnotified.data else 0
        notifier.send_summary(total_saved, restock_count)

    print(f"\nâœ¨ å®Œäº†ï¼åˆè¨ˆ {total_saved} ä»¶ã®æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
