"""
ã¡ã„ã‹ã‚æƒ…å ±åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Twitterã€ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã€ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©ã‹ã‚‰æƒ…å ±ã‚’è‡ªå‹•åé›†
"""
import os
import sys
import time
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
    from bs4 import BeautifulSoup
    import feedparser
    from supabase import create_client, Client
except ImportError as e:
    print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    print("ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("pip install requests beautifulsoup4 feedparser supabase")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ========================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================

def generate_source_id(url: str) -> str:
    """URLã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(url.encode()).hexdigest()

def classify_content(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ¤å®š"""
    keywords = {
        "ã‚°ãƒƒã‚º": ["ã‚°ãƒƒã‚º", "ç™ºå£²", "äºˆç´„", "è²©å£²", "é™å®š", "ã¬ã„ãã‚‹ã¿", "ãƒ•ã‚£ã‚®ãƒ¥ã‚¢", "ãƒã‚¹ã‚³ãƒƒãƒˆ", "ã‚¢ã‚¯ã‚¹ã‚¿"],
        "ãã˜": ["ä¸€ç•ªãã˜", "ãã˜", "ãƒ­ãƒƒãƒˆ", "æ™¯å“"],
        "ã‚¤ãƒ™ãƒ³ãƒˆ": ["ã‚¤ãƒ™ãƒ³ãƒˆ", "é–‹å‚¬", "ã‚³ãƒ©ãƒœ", "ã‚«ãƒ•ã‚§", "ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—", "å±•ç¤º", "ã‚‰ã‚“ã©"],
        "æ¼«ç”»": ["æ›´æ–°", "æ²è¼‰", "é€£è¼‰", "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰", "è©±"],
        "ã‚¢ãƒ‹ãƒ¡": ["æ”¾é€", "é…ä¿¡", "å£°å„ª", "OP", "ED"],
    }
    
    for category, words in keywords.items():
        if any(word in text for word in words):
            return category
    
    return "ãã®ä»–"

def save_to_db(items: List[Dict], source: str) -> int:
    """æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    saved_count = 0
    
    for item in items:
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            existing = supabase.table("information").select("id").eq("source_id", item['source_id']).execute()
            
            if not existing.data:
                data = {
                    "source": source,
                    "source_id": item['source_id'],
                    "title": item['title'],
                    "content": item.get('content', item['title']),
                    "url": item['url'],
                    "images": json.dumps(item.get('images', [])),
                    "category": classify_content(item['title']),
                    "published_at": item.get('published_at', datetime.now().isoformat())
                }
                
                supabase.table("information").insert(data).execute()
                saved_count += 1
                print(f"  âœ… ä¿å­˜: {item['title'][:50]}... (ç”»åƒ{len(item.get('images', []))}æš)")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        time.sleep(0.3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    
    return saved_count

# ========================================
# 1. Twitteråé›†ï¼ˆNitter RSSï¼‰
# ========================================

def collect_twitter() -> List[Dict]:
    """Twitterã‹ã‚‰æƒ…å ±ã‚’å–å¾—"""
    print("\nğŸ¦ Twitteråé›†é–‹å§‹...")
    
    nitter_instances = [
        "https://nitter.poast.org",
        "https://nitter.net",
        "https://nitter.privacydev.net",
    ]
    
    account = "ngnchiikawa"
    
    for instance in nitter_instances:
        try:
            rss_url = f"{instance}/{account}/rss"
            print(f"  è©¦è¡Œä¸­: {instance}")
            
            feed = feedparser.parse(rss_url)
            
            if feed.entries:
                results = []
                for entry in feed.entries[:20]:  # æœ€æ–°20ä»¶
                    # ç”»åƒã‚’å–å¾—
                    images = []
                    
                    # media_contentã‹ã‚‰ç”»åƒå–å¾—
                    if hasattr(entry, 'media_content'):
                        for media in entry.media_content:
                            if 'url' in media:
                                images.append(media['url'])
                    
                    # summaryã‹ã‚‰ç”»åƒURLæŠ½å‡º
                    if hasattr(entry, 'summary'):
                        soup = BeautifulSoup(entry.summary, 'html.parser')
                        img_tags = soup.find_all('img')
                        for img in img_tags:
                            if img.get('src') and img['src'] not in images:
                                images.append(img['src'])
                    
                    # ãƒ„ã‚¤ãƒ¼ãƒˆIDå–å¾—
                    tweet_id = entry.link.split("/")[-1].split("#")[0]
                    
                    results.append({
                        'source_id': f"twitter_{tweet_id}",
                        'title': entry.title,
                        'content': entry.get('summary', entry.title),
                        'url': entry.link.replace(instance, "https://twitter.com"),  # æœ¬å®¶URLã«å¤‰æ›
                        'images': images,
                        'published_at': entry.get('published', '')
                    })
                
                print(f"  âœ… {len(results)}ä»¶å–å¾—æˆåŠŸ")
                return results
                
        except Exception as e:
            print(f"  âŒ {instance}: {e}")
            continue
    
    print("  âŒ ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å¤±æ•—")
    return []

# ========================================
# 2. ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†
# ========================================

def collect_chiikawa_market() -> List[Dict]:
    """ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã‹ã‚‰æ–°å•†å“æƒ…å ±ã‚’å–å¾—"""
    print("\nğŸ ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†é–‹å§‹...")
    
    # æ–°ç€å•†å“ãƒšãƒ¼ã‚¸
    url = "https://chiikawamarket.jp/collections/newitems"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # å•†å“ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        # ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã®å•†å“ã‚«ãƒ¼ãƒ‰
        items = soup.select('.product-item')[:20]  # æœ€æ–°20ä»¶
        
        if not items:
            # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
            items = soup.select('a[href*="/products/"]')[:20]
        
        results = []
        for item in items:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                title_elem = item.select_one('.product-item__title, h3, .product-title')
                if not title_elem:
                    title_elem = item
                
                title = title_elem.get_text(strip=True) if title_elem else "å•†å“"
                
                # URLå–å¾—
                link = item.get('href') if item.name == 'a' else item.select_one('a')
                if not link:
                    continue
                
                product_url = link if isinstance(link, str) else link.get('href')
                if not product_url.startswith('http'):
                    product_url = f"https://chiikawamarket.jp{product_url}"
                
                # ç”»åƒå–å¾—
                images = []
                img_elem = item.select_one('img')
                if img_elem:
                    img_url = img_elem.get('src') or img_elem.get('data-src')
                    if img_url:
                        if not img_url.startswith('http'):
                            img_url = f"https:{img_url}" if img_url.startswith('//') else f"https://chiikawamarket.jp{img_url}"
                        images.append(img_url)
                
                results.append({
                    'source_id': generate_source_id(product_url),
                    'title': title,
                    'content': title,
                    'url': product_url,
                    'images': images,
                    'published_at': datetime.now().isoformat()
                })
                
            except Exception as e:
                print(f"  âš ï¸ é …ç›®ã‚¹ã‚­ãƒƒãƒ—: {e}")
                continue
        
        print(f"  âœ… {len(results)}ä»¶å–å¾—")
        return results
        
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# 3. ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†
# ========================================

def collect_chiikawa_info() -> List[Dict]:
    """ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
    print("\nğŸ“° ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†é–‹å§‹...")
    
    url = "https://chiikawa-info.jp/"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        items = soup.select('.news-item, .event-item, article')[:20]
        
        if not items:
            # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
            items = soup.select('a[href*="chiikawa-info.jp"]')[:20]
        
        results = []
        for item in items:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                title_elem = item.select_one('h2, h3, .title, .event-title')
                if not title_elem:
                    title_elem = item
                
                title = title_elem.get_text(strip=True) if title_elem else "ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±"
                
                if not title or len(title) < 5:
                    continue
                
                # URLå–å¾—
                link = item.get('href') if item.name == 'a' else item.select_one('a')
                if not link:
                    continue
                
                event_url = link if isinstance(link, str) else link.get('href')
                if not event_url.startswith('http'):
                    event_url = f"https://chiikawa-info.jp{event_url}"
                
                # ç”»åƒå–å¾—
                images = []
                img_elems = item.select('img')
                for img in img_elems[:3]:  # æœ€å¤§3æš
                    img_url = img.get('src') or img.get('data-src')
                    if img_url:
                        if not img_url.startswith('http'):
                            img_url = f"https:{img_url}" if img_url.startswith('//') else f"https://chiikawa-info.jp{img_url}"
                        images.append(img_url)
                
                # æ—¥ä»˜å–å¾—
                date_elem = item.select_one('time, .date, .published')
                published = date_elem.get_text(strip=True) if date_elem else datetime.now().isoformat()
                
                results.append({
                    'source_id': generate_source_id(event_url),
                    'title': title,
                    'content': title,
                    'url': event_url,
                    'images': images,
                    'published_at': published
                })
                
            except Exception as e:
                print(f"  âš ï¸ é …ç›®ã‚¹ã‚­ãƒƒãƒ—: {e}")
                continue
        
        print(f"  âœ… {len(results)}ä»¶å–å¾—")
        return results
        
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ­ ã¡ã„ã‹ã‚æƒ…å ±åé›†é–‹å§‹")
    print("=" * 60)
    print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    total_saved = 0
    
    # 1. Twitter
    twitter_items = collect_twitter()
    if twitter_items:
        saved = save_to_db(twitter_items, "twitter")
        print(f"  ğŸ“Š Twitter: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    time.sleep(2)
    
    # 2. ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ
    market_items = collect_chiikawa_market()
    if market_items:
        saved = save_to_db(market_items, "chiikawa_market")
        print(f"  ğŸ“Š ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    time.sleep(2)
    
    # 3. ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©
    info_items = collect_chiikawa_info()
    if info_items:
        saved = save_to_db(info_items, "chiikawa_info")
        print(f"  ğŸ“Š ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    
    print("\n" + "=" * 60)
    print(f"âœ¨ åé›†å®Œäº†ï¼åˆè¨ˆ {total_saved} ä»¶ã®æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    print("=" * 60)

if __name__ == "__main__":
    main()