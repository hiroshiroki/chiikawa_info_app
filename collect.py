"""
ã¡ã„ã‹ã‚æƒ…å ±åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’è‡ªå‹•åé›†
"""
import os
import sys
import time
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
import re
import pytz

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
    from bs4 import BeautifulSoup
    from supabase import create_client, Client
except ImportError as e:
    print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ========================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================

def generate_source_id(text: str) -> str:
    """æ–‡å­—åˆ—ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(text.encode()).hexdigest()

def save_to_db(items: List[Dict], source: str) -> int:
    """æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    saved_count = 0
    
    # æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒå…ˆã«æ¥ã‚‹ã‚ˆã†ã«é€†é †ã§å‡¦ç†
    for item in reversed(items):
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ (source_idã‚’ä½¿ç”¨)
            existing = supabase.table("information").select("id").eq("source_id", item['source_id']).execute()
            
            if not existing.data:
                data = {
                    "source": source,
                    "source_id": item['source_id'],
                    "title": item['title'],
                    "content": item.get('content', item['title']),
                    "url": item['url'],
                    "images": item.get('images', []),
                    "price": item.get('price'),
                    "category": "ã‚°ãƒƒã‚º",
                    "published_at": item.get('published_at', datetime.now(pytz.timezone('Asia/Tokyo')).isoformat()),
                    "status": item.get('status', 'new'),
                    "event_date": item.get('event_date')
                }
                
                supabase.table("information").insert(data).execute()
                saved_count += 1
                img_count = len(item.get('images', []))
                print(f"  âœ… ä¿å­˜: {item['title'][:30]}... (ç”»åƒ{img_count}æš)")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {item.get('title', 'ä¸æ˜ãªã‚¢ã‚¤ãƒ†ãƒ ')} - {e}")
        time.sleep(0.1) 
    return saved_count

# ========================================
# ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†
# ========================================

def get_latest_market_urls() -> Dict[str, Optional[str]]:
    """ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã®æœ€æ–°ã®æ–°å•†å“ãƒ»å†å…¥è·ãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—"""
    print("  ğŸ”— æœ€æ–°ã®ãƒãƒ¼ã‚±ãƒƒãƒˆURLã‚’å–å¾—ä¸­...")
    base_url = "https://chiikawamarket.jp"
    urls = {"new": None, "restock": None}
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(base_url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        link_selectors = [
            'a[href*="/collections/newitems"]', 
            'a[href*="/collections/restock"]',
            'a:contains("æ–°å•†å“")',
            'a:contains("å†å…¥è·")'
        ]
        
        for selector in link_selectors:
            for link in soup.select(selector):
                href = link.get('href')
                if not href: continue
                
                full_url = f"{base_url}{href}" if not href.startswith('http') else href
                
                if "newitems" in href or "æ–°å•†å“" in link.get_text(strip=True):
                    urls["new"] = full_url
                elif "restock" in href or "å†å…¥è·" in link.get_text(strip=True):
                    urls["restock"] = full_url
        
        # ã‚‚ã—è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not urls["new"]:
            urls["new"] = "https://chiikawamarket.jp/collections/newitems"
        if not urls["restock"]:
            urls["restock"] = "https://chiikawamarket.jp/collections/restock"

        print(f"  ğŸ‘ å–å¾—æˆåŠŸ: NEW -> {urls['new']}, RESTOCK -> {urls['restock']}")
        return urls
    except Exception as e:
        print(f"  âŒ æœ€æ–°ãƒãƒ¼ã‚±ãƒƒãƒˆURLã®å–å¾—ã«å¤±æ•—: {e}")
        return {
            "new": "https://chiikawamarket.jp/collections/newitems",
            "restock": "https://chiikawamarket.jp/collections/restock"
        }

def collect_chiikawa_market(url: str, status: str) -> List[Dict]:
    if not url:
        print(f"{(f' ({status})')} ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []
    print(f"{(f' ({status})')} åé›†é–‹å§‹: {url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        event_date_str = None
        date_header = soup.select_one('.collection__title, .section-header__title, h1')
        if date_header:
            text = date_header.get_text(strip=True)
            match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
            if match:
                month, day = int(match.group(1)), int(match.group(2))
                now = datetime.now(pytz.timezone('Asia/Tokyo'))
                year = now.year if now.month >= month else now.year -1
                try:
                    event_date_str = datetime(year, month, day).strftime('%Y-%m-%d')
                    print(f"  ğŸ“… ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ã‚’æŠ½å‡º: {event_date_str} (from: {text})")
                except ValueError:
                    event_date_str = None
        
        results = []
        # å•†å“ã‚«ãƒ¼ãƒ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ã‚ˆã‚Šå …ç‰¢ã«
        items = soup.select('.card-wrapper, .product-grid .grid__item')

        for item in items:
            title_elem = item.select_one('.card__heading, .card-information__text')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            
            link_elem = item.select_one('a[href*="/products/"]')
            if not link_elem: continue
            product_url = link_elem.get('href')
            if not product_url.startswith('http'):
                product_url = f"https://chiikawamarket.jp{product_url.split('?')[0]}"

            source_id = generate_source_id(f"{product_url}_{title}")

            images = []
            img_tag = item.select_one('.card__media img, .media img')
            if img_tag:
                img_url = img_tag.get('src') or img_tag.get('data-src')
                if not img_url and img_tag.get('srcset'):
                    img_url = re.split(r'\s*,\s*', img_tag.get('srcset'))[0].split(' ')[0]
                
                if img_url:
                    if img_url.startswith('//'): img_url = f"https:{img_url}"
                    images.append(img_url.split('?')[0])

            price = None
            price_elem = item.select_one('.price__regular .price-item, .price-item--regular')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                match = re.search(r'(\d[\d,.]*)', price_text)
                if match:
                    try:
                        price = int(float(match.group(1).replace(',', '')))
                    except ValueError:
                        price = None
            
            # æ–°ã—ã„å•†å“ãŒãƒªã‚¹ãƒˆã®ä¸Šéƒ¨ã«ã‚ã‚‹ã¨ä»®å®šã—ã€é †ã«å–å¾—
            results.append({
                'source_id': source_id,
                'title': title,
                'url': product_url,
                'images': images,
                'price': price,
                'published_at': datetime.now(pytz.timezone('Asia/Tokyo')).isoformat(),
                'status': status,
                'event_date': event_date_str
            })
            if len(results) >= 50: break # åé›†æ•°ã®ä¸Šé™

        print(f"  âœ… {len(results)}ä»¶è§£æå®Œäº†")
        return results
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    print(f"ğŸš€ å®Ÿè¡Œé–‹å§‹: {datetime.now(pytz.timezone('Asia/Tokyo'))}")
    total_saved = 0
    
    market_urls = get_latest_market_urls()

    all_items = []
    if market_urls.get("new"):
        print("\n--- chiikawa_market (new) åé›† ---")
        all_items.extend(collect_chiikawa_market(market_urls["new"], "new"))
        time.sleep(1)

    if market_urls.get("restock"):
        print("\n--- chiikawa_market (restock) åé›† ---")
        all_items.extend(collect_chiikawa_market(market_urls["restock"], "restock"))
        time.sleep(1)

    if all_items:
        # ã‚µã‚¤ãƒˆä¸Šã§æ–°ã—ã„ã‚‚ã®ãŒä¸Šã«ã‚ã‚‹ãŸã‚ã€å–å¾—ã—ãŸãƒªã‚¹ãƒˆã®é€†é †ï¼ˆå¤ã„ã‚‚ã®ï¼‰ã‹ã‚‰DBã«ä¿å­˜ã—ã¦ã„ã
        # ã“ã‚Œã«ã‚ˆã‚Šã€Webã‚µã‚¤ãƒˆã§ã®è¡¨ç¤ºé †ï¼ˆæ–°ã—ã„ã‚‚ã®ãŒä¸Šï¼‰ã¨DBã®ä¿å­˜é †ãŒä¸€è‡´ã—ã‚„ã™ããªã‚‹
        print("\n--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ ---")
        saved = save_to_db(all_items, "chiikawa_market")
        print(f"  ğŸ“Š chiikawa_market: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print("  âš ï¸ chiikawa_marketã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    print(f"\nâœ¨ å®Œäº†ï¼åˆè¨ˆ {total_saved} ä»¶ã®æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
