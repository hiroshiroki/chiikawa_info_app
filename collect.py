"""
ã¡ã„ã‹ã‚æƒ…å ±åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Twitter(Nitter), ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ, ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©ã‹ã‚‰æƒ…å ±ã‚’è‡ªå‹•åé›†
"""
import os
import sys
import time
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
import re
import pytz

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
    from bs4 import BeautifulSoup
    import snscrape.modules.twitter as sntwitter
    from supabase import create_client, Client
except ImportError as e:
    print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ========================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================

def generate_source_id(text: str) -> str:
    """æ–‡å­—åˆ—ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(text.encode()).hexdigest()

def classify_content(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ¤å®š"""
    keywords = {
        "ã‚°ãƒƒã‚º": ["ã‚°ãƒƒã‚º", "ç™ºå£²", "äºˆç´„", "è²©å£²", "é™å®š", "ã¬ã„ãã‚‹ã¿", "ãƒ•ã‚£ã‚®ãƒ¥ã‚¢", "ãƒã‚¹ã‚³ãƒƒãƒˆ", "ã‚¢ã‚¯ã‚¹ã‚¿"],
        "ãã˜": ["ä¸€ç•ªãã˜", "ãã˜", "ãƒ­ãƒƒãƒˆ", "æ™¯å“"],
        "ã‚¤ãƒ™ãƒ³ãƒˆ": ["ã‚¤ãƒ™ãƒ³ãƒˆ", "é–‹å‚¬", "ã‚³ãƒ©ãƒœ", "ã‚«ãƒ•ã‚§", "ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—", "å±•ç¤º", "ã‚‰ã‚“ã©"],
        "æ¼«ç”»": ["æ›´æ–°", "æ²è¼‰", "é€£è¼‰", "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰", "è©±"],
        "ã‚¢ãƒ‹ãƒ¡": ["æ”¾é€", "é…ä¿¡", "å£°å„ª", "OP", "ED"],
    }
    
    for category, words in keywords.items():
        if any(word in text for word in words):
            return category
    
    return "ãã®ä»–"

def save_to_db(items: List[Dict], source: str) -> int:
    """æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    saved_count = 0
    
    for item in items:
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ (source_idã‚’ä½¿ç”¨)
            existing = supabase.table("information").select("id").eq("source_id", item['source_id']).execute()
            
            if not existing.data:
                category = "ã‚°ãƒƒã‚º" if source == "chiikawa_market" else classify_content(item['title'])

                data = {
                    "source": source,
                    "source_id": item['source_id'],
                    "title": item['title'],
                    "content": item.get('content', item['title']),
                    "url": item['url'],
                    "images": item.get('images', []),
                    "price": item.get('price'),
                    "category": category,
                    "published_at": item.get('published_at', datetime.now(pytz.timezone('Asia/Tokyo')).isoformat()),
                    "status": item.get('status', 'new'),
                    "event_date": item.get('event_date') # ã“ã“ã§è¿½åŠ 
                }
                
                supabase.table("information").insert(data).execute()
                saved_count += 1
                img_count = len(item.get('images', []))
                print(f"  âœ… ä¿å­˜: {item['title'][:30]}... (ç”»åƒ{img_count}æš)")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        time.sleep(0.1) 
    return saved_count

# ========================================
# 1. Twitteråé›†ï¼ˆNitter RSSï¼‰
# ========================================

def collect_twitter() -> List[Dict]:
    print("\nğŸ¦ Twitteråé›†é–‹å§‹ (snscrapeä½¿ç”¨)...")
    results = []
    account = "chiikawasan"
    max_tweets = 20 # åé›†ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆã®æœ€å¤§æ•°

    try:
        # snscrapeã‚’ä½¿ã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—
        scraper = sntwitter.TwitterUserScraper(account)
        
        for i, tweet in enumerate(scraper.get_items()):
            if i >= max_tweets:
                break

            images = []
            if tweet.media:
                for medium in tweet.media:
                    if isinstance(medium, sntwitter.Photo):
                        # 'orig' or 'large' ã‚µã‚¤ã‚ºã®URLã‚’å–å¾—
                        images.append(medium.fullUrl.replace('name=large', 'name=orig'))
                    elif isinstance(medium, sntwitter.Video):
                        # ãƒ“ãƒ‡ã‚ªã®ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’å–å¾—
                        images.append(medium.thumbnailUrl)

            # æœ¬æ–‡ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
            title = tweet.rawContent
            if len(title) > 100:
                title = title[:97] + "..."

            results.append({
                'source_id': f"twitter_{tweet.id}",
                'title': title,
                'content': tweet.rawContent,
                'url': tweet.url,
                'images': images,
                'published_at': tweet.date.astimezone(pytz.timezone('Asia/Tokyo')).isoformat(),
            })
        
        print(f"  âœ… {len(results)}ä»¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’è§£æ")
        return results

    except Exception as e:
        print(f"  âŒ snscrapeã§ã®åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        # snscrapeãŒå¤±æ•—ã—ãŸå ´åˆã€åé›†ã‚’ä¸­æ­¢
        return []

# ========================================
# 2. ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†ï¼ˆå¼·åŒ–ç‰ˆï¼‰
# ========================================

def get_latest_market_urls() -> Dict[str, Optional[str]]:
    """ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã®æœ€æ–°ã®æ–°å•†å“ãƒ»å†å…¥è·ãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—"""
    print("  ğŸ”— æœ€æ–°ã®ãƒãƒ¼ã‚±ãƒƒãƒˆURLã‚’å–å¾—ä¸­...")
    base_url = "https://chiikawamarket.jp"
    urls = {"new": None, "restock": None}
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(base_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        nav_links = soup.select('a.nav-link, .header__menu-item')
        for link in nav_links:
            text = link.get_text(strip=True)
            href = link.get('href')
            if not href: continue

            if "æ–°å•†å“" in text or "NEW" in text:
                urls["new"] = f"{base_url}{href}" if not href.startswith('http') else href
            elif "å†å…¥è·" in text or "RESTOCK" in text:
                urls["restock"] = f"{base_url}{href}" if not href.startswith('http') else href
        
        print(f"  ğŸ‘ å–å¾—æˆåŠŸ: NEW -> {urls['new']}, RESTOCK -> {urls['restock']}")
        return urls
    except Exception as e:
        print(f"  âŒ æœ€æ–°ãƒãƒ¼ã‚±ãƒƒãƒˆURLã®å–å¾—ã«å¤±æ•—: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä»¥å‰ã®URLã‚’è¿”ã™
        return {
            "new": "https://chiikawamarket.jp/collections/newitems",
            "restock": "https://chiikawamarket.jp/collections/restock"
        }

def collect_chiikawa_market(url: str, status: str) -> List[Dict]:
    if not url:
        print(f"\nğŸ ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ ({status}) ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []
    print(f"\nğŸ ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ ({status}) åé›†é–‹å§‹: {url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º (ä¾‹: "1æœˆ30æ—¥ç™ºå£²å•†å“"ã€"1æœˆ23æ—¥å†å…¥è·å•†å“")
        event_date_str = None
        date_header = soup.select_one('h1.page-title, .collection__title')
        if date_header:
            match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', date_header.get_text())
            if match:
                month, day = int(match.group(1)), int(match.group(2))
                now = datetime.now(pytz.timezone('Asia/Tokyo'))
                # å¹´ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: æŠ½å‡ºã—ãŸæœˆãŒæœªæ¥ã®æœˆãªã‚‰å»å¹´ã€ãã†ã§ãªã‘ã‚Œã°ä»Šå¹´
                year = now.year - 1 if now.month < month else now.year
                try:
                    event_date_str = datetime(year, month, day).strftime('%Y-%m-%d')
                    print(f"  ğŸ“… ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ã‚’æŠ½å‡º: {event_date_str}")
                except ValueError:
                    event_date_str = None # 2/30ã®ã‚ˆã†ãªä¸æ­£ãªæ—¥ä»˜ã¯Noneã«
                
        items = soup.select('.product-item, .card-wrapper')
        results = []
        seen_ids = set()

        for item in items:
            title_elem = item.select_one('.product-item__title, .card__heading, h3.card-information__text')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            
            link_elem = item.select_one('a[href*="/products/"]')
            if not link_elem: continue
            product_url = link_elem.get('href')
            if not product_url.startswith('http'):
                product_url = f"https://chiikawamarket.jp{product_url}"

            source_id = generate_source_id(f"{product_url}_{title}")
            if source_id in seen_ids: continue
            seen_ids.add(source_id)

            images = []
            img_tag = item.select_one('img.media')
            if img_tag:
                # data-src, src, srcset ã®é †ã§è©¦ã™
                img_url = img_tag.get('src') or img_tag.get('data-src')
                if not img_url and img_tag.get('srcset'):
                    # srcsetã‹ã‚‰æœ€åˆã®URLã‚’å–å¾—
                    img_url = re.split(r'\s*,\s*', img_tag.get('srcset'))[0].split(' ')[0]
                
                if img_url:
                    if img_url.startswith('//'): img_url = f"https:{img_url}"
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã€å®Œå…¨ãªURLã«å¤‰æ›
                    elif not img_url.startswith('http'):
                        img_url = f"https:{img_url}" if img_url.startswith('//') else f"https://chiikawamarket.jp{img_url}"

                    images.append(img_url.split('?')[0])


            price = None
            price_elem = item.select_one('.price, .price-item')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                try:
                    # "Â¥1,100" ã‚„ "1100" ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰æ•°å­—ã®ã¿ã‚’æŠ½å‡º
                    match = re.search(r'(\d[\d,.]*)', price_text)
                    if match:
                        price = int(float(match.group(1).replace(',', '')))
                except (ValueError, IndexError):
                    price = None

            results.append({
                'source_id': source_id,
                'title': title,
                'url': product_url,
                'images': images,
                'price': price,
                'published_at': datetime.now(pytz.timezone('Asia/Tokyo')).isoformat(),
                'status': status,
                'event_date': event_date_str # ã“ã“ã§è¿½åŠ 
            })
            if len(results) >= 50: break

        print(f"  âœ… {len(results)}ä»¶è§£æå®Œäº†")
        return results
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# 3. ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†
# ========================================

def collect_chiikawa_info() -> List[Dict]:
    print("\nğŸ“° ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†é–‹å§‹...")
    url = "https://chiikawa-info.jp/"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        items = soup.select('article.post-item')
        results = []
        for item in items:
            title_elem = item.select_one('h2.post-title')
            link_elem = item.select_one('a.post-link')
            if not title_elem or not link_elem: continue
            
            title = title_elem.get_text(strip=True)
            info_url = link_elem.get('href')
            if not info_url or len(title) < 5: continue
            if not info_url.startswith('http'):
                info_url = f"https://chiikawa-info.jp{info_url}"

            images = []
            img_tag = item.select_one('img.post-thumb-img')
            if img_tag:
                src = img_tag.get('src')
                if src:
                    if src.startswith('//'): src = f"https:{src}"
                    elif not src.startswith('http'): src = f"https://chiikawa-info.jp{src}"
                    images.append(src)

            # published_at ã‚’è¨˜äº‹ã®æ—¥ä»˜ã‹ã‚‰å–å¾—
            date_elem = item.select_one('time.post-date')
            published_at = datetime.now(pytz.timezone('Asia/Tokyo')).isoformat()
            if date_elem and date_elem.get('datetime'):
                try:
                    published_at = datetime.fromisoformat(date_elem.get('datetime')).astimezone(pytz.timezone('Asia/Tokyo')).isoformat()
                except ValueError:
                    pass # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ç¾åœ¨æ™‚åˆ»ã®ã¾ã¾

            results.append({
                'source_id': generate_source_id(info_url),
                'title': title,
                'url': info_url,
                'images': images,
                'published_at': published_at,
            })
            if len(results) >= 20: break
        
        print(f"  âœ… {len(results)}ä»¶è§£æå®Œäº†")
        return results
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    print(f"ğŸš€ å®Ÿè¡Œé–‹å§‹: {datetime.now(pytz.timezone('Asia/Tokyo'))}")
    total_saved = 0
    
    # Twitter
    print("\n--- twitter åé›† ---")
    items = collect_twitter()
    if items:
        saved = save_to_db(items, "twitter")
        print(f"  ğŸ“Š twitter: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print(f"  âš ï¸ twitter ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    time.sleep(1)

    # ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã®URLã‚’å‹•çš„ã«å–å¾—
    market_urls = get_latest_market_urls()

    # ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆï¼ˆæ–°å•†å“ï¼‰
    if market_urls.get("new"):
        print("\n--- chiikawa_market (new) åé›† ---")
        market_new_items = collect_chiikawa_market(market_urls["new"], "new")
        if market_new_items:
            saved = save_to_db(market_new_items, "chiikawa_market")
            print(f"  ğŸ“Š chiikawa_market (new): {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
            total_saved += saved
        else:
            print(f"  âš ï¸ chiikawa_market (new) ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        time.sleep(1)

    # ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆï¼ˆå†å…¥è·ï¼‰
    if market_urls.get("restock"):
        print("\n--- chiikawa_market (restock) åé›† ---")
        market_restock_items = collect_chiikawa_market(market_urls["restock"], "restock")
        if market_restock_items:
            saved = save_to_db(market_restock_items, "chiikawa_market")
            print(f"  ğŸ“Š chiikawa_market (restock): {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
            total_saved += saved
        else:
            print(f"  âš ï¸ chiikawa_market (restock) ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        time.sleep(1)

    # ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©
    print("\n--- chiikawa_info åé›† ---")
    info_items = collect_chiikawa_info()
    if info_items:
        saved = save_to_db(info_items, "chiikawa_info")
        print(f"  ğŸ“Š chiikawa_info: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print(f"  âš ï¸ chiikawa_info ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    time.sleep(1)

    print(f"\nâœ¨ å®Œäº†ï¼åˆè¨ˆ {total_saved} ä»¶ã®æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
