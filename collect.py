"""
ã¡ã„ã‹ã‚æƒ…å ±åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Twitter(Nitter), ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ, ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©ã‹ã‚‰æƒ…å ±ã‚’è‡ªå‹•åé›†
"""
import os
import sys
import time
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
import re

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
    from bs4 import BeautifulSoup
    import feedparser
    from supabase import create_client, Client
except ImportError as e:
    print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ========================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================

def generate_source_id(text: str) -> str:
    """æ–‡å­—åˆ—ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(text.encode()).hexdigest()

def classify_content(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ¤å®š"""
    keywords = {
        "ã‚°ãƒƒã‚º": ["ã‚°ãƒƒã‚º", "ç™ºå£²", "äºˆç´„", "è²©å£²", "é™å®š", "ã¬ã„ãã‚‹ã¿", "ãƒ•ã‚£ã‚®ãƒ¥ã‚¢", "ãƒã‚¹ã‚³ãƒƒãƒˆ", "ã‚¢ã‚¯ã‚¹ã‚¿"],
        "ãã˜": ["ä¸€ç•ªãã˜", "ãã˜", "ãƒ­ãƒƒãƒˆ", "æ™¯å“"],
        "ã‚¤ãƒ™ãƒ³ãƒˆ": ["ã‚¤ãƒ™ãƒ³ãƒˆ", "é–‹å‚¬", "ã‚³ãƒ©ãƒœ", "ã‚«ãƒ•ã‚§", "ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—", "å±•ç¤º", "ã‚‰ã‚“ã©"],
        "æ¼«ç”»": ["æ›´æ–°", "æ²è¼‰", "é€£è¼‰", "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰", "è©±"],
        "ã‚¢ãƒ‹ãƒ¡": ["æ”¾é€", "é…ä¿¡", "å£°å„ª", "OP", "ED"],
    }
    
    for category, words in keywords.items():
        if any(word in text for word in words):
            return category
    
    return "ãã®ä»–"

def save_to_db(items: List[Dict], source: str) -> int:
    """æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    saved_count = 0
    
    for item in items:
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ (source_idã‚’ä½¿ç”¨)
            existing = supabase.table("information").select("id").eq("source_id", item['source_id']).execute()
            
            if not existing.data:
                category = "ã‚°ãƒƒã‚º" if source == "chiikawa_market" else classify_content(item['title'])

                data = {
                    "source": source,
                    "source_id": item['source_id'],
                    "title": item['title'],
                    "content": item.get('content', item['title']),
                    "url": item['url'],
                    "images": item.get('images', []),
                    "price": item.get('price'),
                    "category": category,
                    "published_at": item.get('published_at', datetime.now().isoformat()),
                    "status": item.get('status', 'new'),
                    "event_date": item.get('event_date') # ã“ã“ã§è¿½åŠ 
                }
                
                supabase.table("information").insert(data).execute()
                saved_count += 1
                img_count = len(item.get('images', []))
                print(f"  âœ… ä¿å­˜: {item['title'][:30]}... (ç”»åƒ{img_count}æš)")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        time.sleep(0.1) 
    return saved_count

# ========================================
# 1. Twitteråé›†ï¼ˆNitter RSSï¼‰
# ========================================

def collect_twitter() -> List[Dict]:
    print("\nğŸ¦ Twitteråé›†é–‹å§‹...")
    nitter_instances = [
        "https://nitter.mint.lgbt", 
        "https://nitter.io", 
        "https://nitter.namazso.eu",
        "https://nitter.bus-hit.me"
    ]
    account = "chiikawasan"
    
    for instance in nitter_instances:
        try:
            rss_url = f"{instance}/{account}/rss"
            print(f"  è©¦è¡Œ: {rss_url}")
            # User-Agentã‚’è¨­å®šã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’å›é¿
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            feed_content = requests.get(rss_url, headers=headers, timeout=10).content
            feed = feedparser.parse(feed_content)

            if feed.entries:
                results = []
                for entry in feed.entries[:20]:
                    images = []
                    if hasattr(entry, 'summary'):
                        soup = BeautifulSoup(entry.summary, 'html.parser')
                        # /pic/media%2F... ã®ã‚ˆã†ãªå½¢å¼ã®ç”»åƒã‚’æŠ½å‡º
                        for a_tag in soup.find_all('a', href=lambda href: href and '/pic/media' in href):
                            img_path = a_tag['href']
                            # URLã‚’å†æ§‹ç¯‰
                            img_url = f"{instance}{img_path}"
                            images.append(img_url)

                        # å¾“æ¥ã®imgã‚¿ã‚°ã‚‚ä¸€å¿œãƒã‚§ãƒƒã‚¯
                        for img in soup.find_all('img'):
                            src = img.get('src')
                            if src:
                                if src.startswith('//'): src = f"https:{src}"
                                # ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒãªã‘ã‚Œã°ä»˜ä¸
                                if not src.startswith('http'):
                                     src = f"{instance}{src}"
                                if src not in images: # é‡è¤‡ã‚’é¿ã‘ã‚‹
                                    images.append(src)
                    
                    # Nitterã®URLã‚’Twitterã®URLã«å¤‰æ›
                    tweet_link = entry.link
                    if "nitter" in tweet_link:
                         tweet_link = tweet_link.replace(instance, "https://twitter.com")

                    tweet_id = tweet_link.split("/")[-1].split("#")[0]
                    
                    results.append({
                        'source_id': f"twitter_{tweet_id}",
                        'title': entry.title[:100],
                        'content': entry.get('summary', entry.title),
                        'url': tweet_link,
                        'images': images,
                        'published_at': datetime.now().isoformat()
                    })
                print(f"  âœ… {len(results)}ä»¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’è§£æ")
                return results
        except Exception as e:
            print(f"  âŒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼ ({instance}): {e}")
            continue
    print("  âš ï¸ å…¨ã¦ã®Nitterã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§åé›†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    return []

# ========================================
# 2. ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†ï¼ˆå¼·åŒ–ç‰ˆï¼‰
# ========================================

def collect_chiikawa_market(url: str, status: str) -> List[Dict]:
    print(f"\nğŸ ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆ ({status}) åé›†é–‹å§‹...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º (ä¾‹: "1æœˆ30æ—¥ç™ºå£²å•†å“"ã€"1æœˆ23æ—¥å†å…¥è·å•†å“")
        event_date_str = None
        date_header = soup.select_one('h1.page-title, h2.section-header__title')
        if date_header:
            match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', date_header.get_text())
            if match:
                month = int(match.group(1))
                day = int(match.group(2))
                # ä»Šå¹´ã¾ãŸã¯æ¥å¹´ã®æ—¥ä»˜ã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
                now = datetime.now()
                try:
                    # ä»Šå¹´ã®æ—¥ä»˜ã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
                    event_date_candidate = datetime(now.year, month, day)
                    if event_date_candidate <= now: # ä»Šæ—¥ä»¥å‰ãªã‚‰ã“ã®æ—¥ä»˜ã‚’æ¡ç”¨
                        event_date_str = event_date_candidate.strftime('%Y-%m-%d')
                    else: # æœªæ¥ã®æ—¥ä»˜ãªã‚‰å»å¹´ã®æ—¥ä»˜ã‚’è©¦ã™
                        event_date_candidate = datetime(now.year - 1, month, day)
                        if event_date_candidate <= now:
                            event_date_str = event_date_candidate.strftime('%Y-%m-%d')
                except ValueError:
                    # ç„¡åŠ¹ãªæ—¥ä»˜ï¼ˆä¾‹: 2æœˆ30æ—¥ï¼‰ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    pass
                
        items = soup.select('.product-item, .card')
        results = []
        seen_ids = set()

        for item in items:
            title_elem = item.select_one('.product-item__title, .card__title, h3')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            
            link_elem = item.select_one('a[href*="/products/"]')
            if not link_elem: continue
            product_url = link_elem.get('href')
            if not product_url.startswith('http'):
                product_url = f"https://chiikawamarket.jp{product_url}"

            source_id = generate_source_id(f"{product_url}_{title}")
            if source_id in seen_ids: continue
            seen_ids.add(source_id)

            images = []
            img_tag = item.select_one('img')
            if img_tag:
                img_url = img_tag.get('data-src') or img_tag.get('src') or img_tag.get('data-lazy-src')
                if not img_url and img_tag.get('srcset'):
                    img_url = img_tag.get('srcset').split(',')[0].split(' ')[0]
                
                if img_url:
                    if img_url.startswith('//'): img_url = f"https:{img_url}"
                    elif not img_url.startswith('http'): img_url = f"https://chiikawamarket.jp{img_url}"
                    images.append(img_url.split('?')[0])

            price = None
            price_elem = item.select_one('.price, .price-item')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                try:
                    match = re.search(r'(\d{1,3}(,\d{3})*|\d+)', price_text)
                    if match:
                        price = int(match.group(1).replace(',', ''))
                    else:
                        price = None
                except ValueError:
                    price = None

            results.append({
                'source_id': source_id,
                'title': title,
                'url': product_url,
                'images': images,
                'price': price,
                'published_at': datetime.now().isoformat(),
                'status': status,
                'event_date': event_date_str # ã“ã“ã§è¿½åŠ 
            })
            if len(results) >= 50: break

        print(f"  âœ… {len(results)}ä»¶è§£æå®Œäº†")
        return results
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# 3. ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†
# ========================================

def collect_chiikawa_info() -> List[Dict]:
    print("\nğŸ“° ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©åé›†é–‹å§‹...")
    url = "https://chiikawa-info.jp/"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        items = soup.select('.news-item, article, li')
        results = []
        for item in items:
            title_elem = item.select_one('h2, h3, .title')
            link_elem = item.select_one('a')
            if not title_elem or not link_elem: continue
            
            title = title_elem.get_text(strip=True)
            info_url = link_elem.get('href')
            if not info_url or len(title) < 5: continue
            if not info_url.startswith('http'):
                info_url = f"https://chiikawa-info.jp{info_url}"

            images = []
            img_tag = item.select_one('img')
            if img_tag:
                src = img_tag.get('src')
                if src:
                    if src.startswith('//'): src = f"https:{src}"
                    elif not src.startswith('http'): src = f"https://chiikawa-info.jp{src}"
                    images.append(src)

            results.append({
                'source_id': generate_source_id(info_url),
                'title': title,
                'url': info_url,
                'images': images,
                'published_at': datetime.now().isoformat()
            })
            if len(results) >= 20: break
        return results
    except Exception as e:
        return []

# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    print(f"ğŸš€ å®Ÿè¡Œé–‹å§‹: {datetime.now()}")
    total_saved = 0
    
    # Twitter
    print("\n--- twitter åé›† ---")
    items = collect_twitter()
    if items:
        saved = save_to_db(items, "twitter")
        print(f"  ğŸ“Š twitter: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print(f"  âš ï¸ twitter ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    time.sleep(1)

    # ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆï¼ˆæ–°å•†å“ï¼‰
    print("\n--- chiikawa_market (new) åé›† ---")
    market_new_items = collect_chiikawa_market("https://chiikawamarket.jp/collections/newitems", "new")
    if market_new_items:
        saved = save_to_db(market_new_items, "chiikawa_market")
        print(f"  ğŸ“Š chiikawa_market (new): {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print(f"  âš ï¸ chiikawa_market (new) ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    time.sleep(1)

    # ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆï¼ˆå†å…¥è·ï¼‰
    print("\n--- chiikawa_market (restock) åé›† ---")
    market_restock_items = collect_chiikawa_market("https://chiikawamarket.jp/collections/restock", "restock")
    if market_restock_items:
        saved = save_to_db(market_restock_items, "chiikawa_market")
        print(f"  ğŸ“Š chiikawa_market (restock): {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print(f"  âš ï¸ chiikawa_market (restock) ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    time.sleep(1)

    # ã¡ã„ã‹ã‚ã‚¤ãƒ³ãƒ•ã‚©
    print("\n--- chiikawa_info åé›† ---")
    info_items = collect_chiikawa_info()
    if info_items:
        saved = save_to_db(info_items, "chiikawa_info")
        print(f"  ğŸ“Š chiikawa_info: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print(f"  âš ï¸ chiikawa_info ã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    time.sleep(1)

    print(f"\nâœ¨ å®Œäº†ï¼åˆè¨ˆ {total_saved} ä»¶ã®æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()