"""
ã¡ã„ã‹ã‚æƒ…å ±åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’è‡ªå‹•åé›†
"""
import os
import sys
import time
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
import re
import pytz

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
    from bs4 import BeautifulSoup
    from supabase import create_client, Client
except ImportError as e:
    print(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ========================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================

def generate_source_id(text: str) -> str:
    """æ–‡å­—åˆ—ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(text.encode()).hexdigest()

def save_to_db(items: List[Dict], source: str) -> int:
    """æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    saved_count = 0
    
    # æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒå…ˆã«æ¥ã‚‹ã‚ˆã†ã«é€†é †ã§å‡¦ç†
    for item in reversed(items):
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ (source_idã‚’ä½¿ç”¨)
            existing = supabase.table("information").select("id").eq("source_id", item['source_id']).execute()
            
            if not existing.data:
                data = {
                    "source": source,
                    "source_id": item['source_id'],
                    "title": item['title'],
                    "content": item.get('content', item['title']),
                    "url": item['url'],
                    "images": item.get('images', []),
                    "price": item.get('price'),
                    "category": "ã‚°ãƒƒã‚º",
                    "published_at": item.get('published_at', datetime.now(pytz.timezone('Asia/Tokyo')).isoformat()),
                    "status": item.get('status', 'new'),
                    "event_date": item.get('event_date')
                }
                
                supabase.table("information").insert(data).execute()
                saved_count += 1
                img_count = len(item.get('images', []))
                print(f"  âœ… ä¿å­˜: {item['title'][:30]}... (ç”»åƒ{img_count}æš)")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {item.get('title', 'ä¸æ˜ãªã‚¢ã‚¤ãƒ†ãƒ ')} - {e}")
        time.sleep(0.1) 
    return saved_count

# ========================================
# ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆåé›†
# ========================================

def get_latest_market_urls() -> List[Dict[str, str]]:
    """ã¡ã„ã‹ã‚ãƒãƒ¼ã‚±ãƒƒãƒˆã®æ—¥ä»˜åˆ¥ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—"""
    print("  ğŸ”— æ—¥ä»˜åˆ¥ãƒãƒ¼ã‚±ãƒƒãƒˆURLã‚’å–å¾—ä¸­...")
    base_url = "https://chiikawamarket.jp"
    collections = []
    seen_urls = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨

    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(base_url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
        for link in soup.select('a[href*="/collections/"]'):
            href = link.get('href')
            text = link.get_text(strip=True)

            if not href:
                continue

            full_url = f"{base_url}{href}" if not href.startswith('http') else href

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if full_url in seen_urls:
                continue

            # æ—¥ä»˜ã‚’å«ã‚€æ–°å•†å“ãƒªãƒ³ã‚¯ (ä¾‹: "2æœˆ6æ—¥ç™ºå£²å•†å“" -> /collections/20260206)
            if 'ç™ºå£²å•†å“' in text:
                date_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
                if date_match:
                    collections.append({
                        'url': full_url,
                        'status': 'new',
                        'date_text': text
                    })
                    seen_urls.add(full_url)

            # æ—¥ä»˜ã‚’å«ã‚€å†å…¥è·ãƒªãƒ³ã‚¯ (ä¾‹: "2æœˆ5æ—¥å†å…¥è·å•†å“" -> /collections/re20260205)
            elif 'å†å…¥è·å•†å“' in text and 'å†å…¥è·å•†å“ä¸€è¦§' not in text:
                date_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
                if date_match:
                    collections.append({
                        'url': full_url,
                        'status': 'restock',
                        'date_text': text
                    })
                    seen_urls.add(full_url)

        if collections:
            print(f"  ğŸ‘ å–å¾—æˆåŠŸ: {len(collections)}å€‹ã®æ—¥ä»˜åˆ¥ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹")
            for col in collections:
                print(f"    - {col['date_text']}: {col['url']}")
        else:
            print("  âš ï¸ æ—¥ä»˜åˆ¥ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        return collections

    except Exception as e:
        print(f"  âŒ ãƒãƒ¼ã‚±ãƒƒãƒˆURLã®å–å¾—ã«å¤±æ•—: {e}")
        return []

def collect_chiikawa_market(url: str, status: str, date_text: Optional[str] = None) -> List[Dict]:
    if not url:
        print(f"{(f' ({status})')} ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []
    print(f"{(f' ({status})')} åé›†é–‹å§‹: {url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # æ—¥ä»˜ã®æŠ½å‡º (date_textã¾ãŸã¯URLã‹ã‚‰)
        event_date_str = None

        # 1. date_textã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º (ä¾‹: "2æœˆ6æ—¥ç™ºå£²å•†å“")
        if date_text:
            match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', date_text)
            if match:
                month, day = int(match.group(1)), int(match.group(2))
                now = datetime.now(pytz.timezone('Asia/Tokyo'))
                year = now.year if month <= now.month + 1 else now.year - 1
                try:
                    event_date_str = datetime(year, month, day).strftime('%Y-%m-%d')
                    print(f"  ğŸ“… ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ã‚’æŠ½å‡º: {event_date_str} (from: {date_text})")
                except ValueError:
                    event_date_str = None

        # 2. URLã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º (ä¾‹: /collections/20260206 ã¾ãŸã¯ /collections/re20260205)
        if not event_date_str:
            url_date_match = re.search(r'/collections/(?:re)?(\d{8})', url)
            if url_date_match:
                date_str = url_date_match.group(1)
                try:
                    date_obj = datetime.strptime(date_str, '%Y%m%d')
                    event_date_str = date_obj.strftime('%Y-%m-%d')
                    print(f"  ğŸ“… ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ã‚’æŠ½å‡º: {event_date_str} (from URL: {url})")
                except ValueError:
                    event_date_str = None
        
        results = []
        # å•†å“ã‚«ãƒ¼ãƒ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ã‚ˆã‚Šå …ç‰¢ã«
        items = soup.select('.card-wrapper, .product-grid .grid__item')

        for item in items:
            title_elem = item.select_one('.card__heading, .card-information__text')
            if not title_elem: continue
            title = title_elem.get_text(strip=True)
            
            link_elem = item.select_one('a[href*="/products/"]')
            if not link_elem: continue
            product_url = link_elem.get('href')
            if not product_url.startswith('http'):
                product_url = f"https://chiikawamarket.jp{product_url.split('?')[0]}"

            source_id = generate_source_id(f"{product_url}_{title}")

            images = []
            img_tag = item.select_one('.card__media img, .media img')
            if img_tag:
                img_url = img_tag.get('src') or img_tag.get('data-src')
                if not img_url and img_tag.get('srcset'):
                    img_url = re.split(r'\s*,\s*', img_tag.get('srcset'))[0].split(' ')[0]
                
                if img_url:
                    if img_url.startswith('//'): img_url = f"https:{img_url}"
                    images.append(img_url.split('?')[0])

            price = None
            price_elem = item.select_one('.price__regular .price-item, .price-item--regular')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                match = re.search(r'(\d[\d,.]*)', price_text)
                if match:
                    try:
                        price = int(float(match.group(1).replace(',', '')))
                    except ValueError:
                        price = None
            
            # æ–°ã—ã„å•†å“ãŒãƒªã‚¹ãƒˆã®ä¸Šéƒ¨ã«ã‚ã‚‹ã¨ä»®å®šã—ã€é †ã«å–å¾—
            results.append({
                'source_id': source_id,
                'title': title,
                'url': product_url,
                'images': images,
                'price': price,
                'published_at': datetime.now(pytz.timezone('Asia/Tokyo')).isoformat(),
                'status': status,
                'event_date': event_date_str
            })
            if len(results) >= 50: break # åé›†æ•°ã®ä¸Šé™

        print(f"  âœ… {len(results)}ä»¶è§£æå®Œäº†")
        return results
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    print(f"ğŸš€ å®Ÿè¡Œé–‹å§‹: {datetime.now(pytz.timezone('Asia/Tokyo'))}")
    total_saved = 0

    # æ—¥ä»˜åˆ¥ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    collections = get_latest_market_urls()

    if not collections:
        print("  âš ï¸ åé›†ã™ã‚‹ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    all_items = []
    for collection in collections:
        status_label = "æ–°å•†å“" if collection['status'] == 'new' else "å†å…¥è·"
        print(f"\n--- chiikawa_market ({status_label}: {collection['date_text']}) åé›† ---")
        items = collect_chiikawa_market(
            collection['url'],
            collection['status'],
            collection['date_text']
        )
        all_items.extend(items)
        time.sleep(1)

    if all_items:
        # ã‚µã‚¤ãƒˆä¸Šã§æ–°ã—ã„ã‚‚ã®ãŒä¸Šã«ã‚ã‚‹ãŸã‚ã€å–å¾—ã—ãŸãƒªã‚¹ãƒˆã®é€†é †ï¼ˆå¤ã„ã‚‚ã®ï¼‰ã‹ã‚‰DBã«ä¿å­˜ã—ã¦ã„ã
        # ã“ã‚Œã«ã‚ˆã‚Šã€Webã‚µã‚¤ãƒˆã§ã®è¡¨ç¤ºé †ï¼ˆæ–°ã—ã„ã‚‚ã®ãŒä¸Šï¼‰ã¨DBã®ä¿å­˜é †ãŒä¸€è‡´ã—ã‚„ã™ããªã‚‹
        print("\n--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ ---")
        saved = save_to_db(all_items, "chiikawa_market")
        print(f"  ğŸ“Š chiikawa_market: {saved}ä»¶ã‚’æ–°è¦ä¿å­˜")
        total_saved += saved
    else:
        print("  âš ï¸ chiikawa_marketã‹ã‚‰ã®æ–°è¦æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    print(f"\nâœ¨ å®Œäº†ï¼åˆè¨ˆ {total_saved} ä»¶ã®æ–°è¦æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
